{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.append('../libs')\n",
    "sys.path.append('libs/')\n",
    "\n",
    "import datetime, os\n",
    "import random\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from utils import build_matrix_embeddings as bme, plot_model_performance, logits_to_tokens, report_to_df\n",
    "from transformers import (\n",
    "    TF2_WEIGHTS_NAME,\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    TFBertForTokenClassification,\n",
    "    create_optimizer)\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# ****** DEFINICION DE PARAMETROS *********\n",
    "NUM_WORDS    = 14485 + 2 # 13705\n",
    "LEN_SENTS    = 306\n",
    "MAX_LEN      = 596\n",
    "NUM_TAGS     = 37 + 3\n",
    "NUM_LABELS   = 37 + 3\n",
    "\n",
    "# ****** DEFINICION DE HIPERPARAMETROS *********\n",
    "_EPOCHS      = 15\n",
    "_DROPOUT     = 0.4\n",
    "_BACH_SIZE   = 64\n",
    "VAL_SPLIT    = 0.3\n",
    "NUM_FOLDS    = 10\n",
    "\n",
    "\n",
    "ESPECIAL_TOKEN = 37\n",
    "SEP_TOKEN      = 38\n",
    "PAD_TOKEN      = 39\n",
    "WORD_PAD_TOKEN = 0\n",
    "\n",
    "configuration = BertConfig()\n",
    "BERT_MODEL = \"bert-base-multilingual-cased\"\n",
    "\n",
    "logs_base_dir = \"./logs\"\n",
    "log_dir       = logs_base_dir + \"/\"\n",
    "save_base_dir = './model'\n",
    "save_dir      = save_base_dir + \"/\"\n",
    "\n",
    "\n",
    "le_dicti = {'B-BIOMARKER': 0, 'B-BIOMARKER_STATUS': 1, 'B-CANCER_CONCEPT': 2, 'B-CHEMOTHERAPY': 3, 'B-CLINICAL_SERVICE': 4, 'B-COMORBIDITY': 5, 'B-DRUG': 6, 'B-EXPLICIT_DATE': 7, 'B-FAMILY': 8, 'B-FREQ': 9, 'B-IMPLICIT_DATE': 10, 'B-METRIC': 11, 'B-OCURRENCE_EVENT': 12, 'B-QUANTITY': 13, 'B-RADIOTHERAPY': 14, 'B-STAGE': 15, 'B-SURGERY': 16, 'B-TNM': 17, 'B-TOXIC_HABIT': 18, 'I-BIOMARKER': 19, 'I-BIOMARKER_STATUS': 20, 'I-CANCER_CONCEPT': 21, 'I-CLINICAL_SERVICE': 22, 'I-COMORBIDITY': 23, 'I-DRUG': 24, 'I-EXPLICIT_DATE': 25, 'I-FAMILY': 26, 'I-FREQ': 27, 'I-IMPLICIT_DATE': 28, 'I-METRIC': 29, 'I-OCURRENCE_EVENT': 30, 'I-QUANTITY': 31, 'I-STAGE': 32, 'I-SURGERY': 33, 'I-TNM': 34, 'I-TOXIC_HABIT': 35, 'O': 36, '[CLS]': 37, '[SEP]': 38, '[PAD]': 39}\n",
    "\n",
    "#print(le_dicti )\n",
    "\n",
    "\n",
    "le_dict = {}\n",
    "for key in le_dicti:\n",
    "    #print(key, '->', le_dict[key])\n",
    "    le_dict[le_dicti[key]] = key\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\"bert\": (BertConfig, TFBertForTokenClassification, BertTokenizer)}\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES['bert']\n",
    "config = config_class.from_pretrained(BERT_MODEL, num_labels=NUM_LABELS)\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(BERT_MODEL, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "il_tokens = []\n",
    "\n",
    "def convert_to_input(sentences, tags, in_ou_put, testy):\n",
    "    input_id_list       = []\n",
    "    attention_mask_list = [] \n",
    "    token_type_id_list  = []\n",
    "    len_tokens          = []\n",
    "    len_sentences       = []\n",
    "    \n",
    "    \n",
    "    if in_ou_put == 1:\n",
    "        label_id_list   = []\n",
    "    else:\n",
    "        label_id_list   = 0\n",
    "    \n",
    "    for x,y in tqdm(zip(sentences,tags),total=len(tags)):\n",
    "        tokens = []\n",
    "        \n",
    "        if in_ou_put == 1:\n",
    "            label_ids = []\n",
    "        \n",
    "        for word, label in zip(x, y):\n",
    "            word_tokens = tokenizer.tokenize(str(word))\n",
    "            tokens.extend(word_tokens)\n",
    "            # Use the real label id for the first token of the word, \n",
    "            # and padding ids for the remaining tokens\n",
    "            if in_ou_put == 1:\n",
    "                label_ids.extend([label] + [SEP_TOKEN] * (len(word_tokens) - 1))\n",
    "        \n",
    "        if testy == 1:\n",
    "            il_tokens.append(['[CLS]'] + tokens + (['[PAD]'] * (595 - len(tokens))))\n",
    "        \n",
    "        if in_ou_put == 1:\n",
    "            label_ids = [ESPECIAL_TOKEN] + label_ids + [ESPECIAL_TOKEN]\n",
    "        \n",
    "        if in_ou_put == 1:\n",
    "            len_tokens.append(len(label_ids))\n",
    "            len_sentences.append(len(x))\n",
    "        \n",
    "        inputs = tokenizer.encode_plus(tokens, add_special_tokens=True, max_length=MAX_LEN)\n",
    "        \n",
    "        input_ids       = inputs[\"input_ids\"]\n",
    "        token_type_ids  = inputs[\"token_type_ids\"]\n",
    "        attention_masks = inputs[\"attention_mask\"]\n",
    "        \n",
    "        attention_mask_list.append(attention_masks)\n",
    "        input_id_list.append(input_ids)\n",
    "        token_type_id_list.append(token_type_ids)\n",
    "        \n",
    "        if in_ou_put == 1:\n",
    "            label_id_list.append(label_ids)\n",
    "\n",
    "    input_id_list       = pad_sequences(maxlen=MAX_LEN, sequences=input_id_list,       dtype=\"int32\", padding=\"post\", value=WORD_PAD_TOKEN)\n",
    "    token_type_id_list  = pad_sequences(maxlen=MAX_LEN, sequences=token_type_id_list,  dtype=\"int32\", padding=\"post\")\n",
    "    attention_mask_list = pad_sequences(maxlen=MAX_LEN, sequences=attention_mask_list, dtype=\"int32\", padding=\"post\")\n",
    "    \n",
    "    if in_ou_put == 1:\n",
    "        print(\">>> :\", max(len_tokens))\n",
    "        print(\">>>> :\", max(len_sentences))\n",
    "    \n",
    "    if in_ou_put == 1:\n",
    "        label_id_list   = pad_sequences(maxlen=MAX_LEN, sequences=label_id_list, dtype=\"int32\", padding=\"post\", value=PAD_TOKEN)\n",
    "\n",
    "\n",
    "    return input_id_list, token_type_id_list, attention_mask_list, label_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_samples = [\n",
    "    \"CICLO 2 CARBOPLATINO / PACLITAXEL. \".split(),\n",
    "    \"En Agosto de 2015 ha recibido 3 ciclos de CISPLATINO / VINORELBINA buena tolerancia clinica .\".split(),\n",
    "    \"QT : CISPLATINO 75 mg / m2 DIA 1 IV + VINORELBINA 25 mg / m2 IV DIAS 1,8 - Adenocarcinoma pulmon lobulo superior derecho \".split(),\n",
    "    \"El dia 27 de junio iniciamos tratamiento con quimioterapia segun esquema CARBOPLATINO / PEMETREXED .\".split(),\n",
    "    \"CICLO 1 CARBOPLATINO AUC 5 - PEMETREXED 500 mg/m2 IV cada 21 dias..\".split(),\n",
    "    \"RT con dosis 50 Gy, se encuentra bien. \".split(),\n",
    "    \"Carcinoma escamoso de pulmón cT3 cN2 cM0 (al menos estadio IIIB de TNM 8ª ed .\".split(),\n",
    "    \"Diagnosticado en marzo de 2016 de Adenoca de pulmón cT2cN2cM1a .\".split(),\n",
    "    \"Ha sido diagnosticada de cancer de pulmon en marzo de 2019 .\".split(),\n",
    "    \"Inicia tratamiento con Cisplatino + Pemetrexed + Bevacizumab (5 ciclos administrados, ultimo en enero de 2014).\".split(),\n",
    "    \"Carcinoma escamoso de pulmón intervenido en marzo 2017 .\".split(),\n",
    "    \"En 2014, intervenido de carcinoma de pulmón pT2bN1cM0 realizandose nefrectomia derecha .\".split(),\n",
    "    \"carcinoma microcitico de pulmon t4n2m0 en tto quimioterapico: carboplatino / etoposido .\".split(),\n",
    "    \"Colico renoureteral derecho con fracaso renal obstructivo en Julio de 2015. \".split()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_y_train = []\n",
    "\n",
    "for snt in ner_samples:\n",
    "    senti = []\n",
    "    for wds in snt:\n",
    "        senti.append('-PAD-')\n",
    "    \n",
    "    dummy_y_train.append(senti)\n",
    "\n",
    "demo_input_ids_train, demo_token_ids_train, demo_attention_masks_train, label_ids_train = convert_to_input(ner_samples, dummy_y_train, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(demo_input_ids_train[0]), len(demo_token_ids_train[0]), len(demo_attention_masks_train[0]),label_ids_train )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.saved_model.load(save_dir)\n",
    "#new_model = tf.keras.models.load_model(save_dir)\n",
    "print(\"BERT model loaded succesfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_prediction = new_model([demo_input_ids_train, demo_token_ids_train, demo_attention_masks_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_pred_tags = np.argmax(demo_prediction, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_y_pred = logits_to_tokens(demo_pred_tags, le_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h, oracc in enumerate(ner_samples):\n",
    "    #heads = oracc\n",
    "    #if h == 0:\n",
    "    tokensito = []\n",
    "    for wordi in oracc:\n",
    "        wordi_tokens = tokenizer.tokenize(str(wordi))\n",
    "        tokensito.extend(wordi_tokens)\n",
    "\n",
    "    print(oracc)\n",
    "    #print(tokensito)\n",
    "    #print(demo_y_pred[h])\n",
    "    heads = tokensito\n",
    "    body  = [demo_y_pred[h][1:len(tokensito)+1]]\n",
    "    display(HTML(\"<div style='overflow-x: auto; white-space: nowrap;'>\" + \n",
    "                 tabulate(body, headers=heads, tablefmt=\"html\") + \n",
    "                 \"</div>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
